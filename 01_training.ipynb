{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1527a6f2-7e24-4f6d-9612-f6d8eb076217",
   "metadata": {},
   "source": [
    "# 01 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46810d4-a578-478f-bd30-7a20dd664105",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Use Case: You are working for Octank Bank and you want to develop a classification model that predicts whethere a customer has credit risk or not.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a43f64-f992-4b98-b120-bf2f8ebc5521",
   "metadata": {},
   "source": [
    "In this notebook you will:\n",
    "1. Train an XGBoost model in the Jupyter environment in SageMaker Studio\n",
    "2. Save model artifacts to Amazon S3\n",
    "3. Save the model to the Amazon SageMaker model registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58ed24-2381-40f4-a520-b129ee5f5e4e",
   "metadata": {},
   "source": [
    "Before you run this notebook, make sure you have executed the module `00_setup` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8c3faa-7dd0-48a8-840c-9e688a41aabd",
   "metadata": {},
   "source": [
    "## 1. Set up environment\n",
    "\n",
    "First, let's restore variables from the `00_setup` notebook and import the data science libraries required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72b1f7-59f1-49ca-aef8-eb8d32d2b3f0",
   "metadata": {},
   "source": [
    "You will also initialise the following boto3 clients:\n",
    "\n",
    "- _s3_client_ for managing storage\n",
    "- _sagemaker_client_ for model operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845690d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_data_path test_data_path\n",
    "%store -r bucket_name model_prefix role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced0440-9efc-4142-b01d-95c86151aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import tarfile\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# Initialize boto3 clients\n",
    "s3_client = boto3.client('s3')\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region=session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4763bea-6fed-4bf3-980c-7b4557962408",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Perform feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2561c-20b9-48f7-8189-3572e22f5bbe",
   "metadata": {},
   "source": [
    "In this step, you'll load CSV file with the training data set you created in the`00_setup` notebook. This file contains the features and target variable that you'll use to train the XGBoost model for credit risk prediction. Let's examine the structure of the dataset to confirm:\n",
    "\n",
    "- The number of records (rows) available for training\n",
    "- The total number of features (columns)\n",
    "- The data types of each feature\n",
    "- Potential missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c71f5-e251-4196-9bc5-a42145c3f518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(train_data_path):\n",
    "    df = pd.read_csv(train_data_path)\n",
    "    \n",
    "    # Display information about the DataFrame\n",
    "    print(f\"Data loaded successfully from {train_data_path}\")\n",
    "    print(f\"DataFrame shape: {df.shape} (rows, columns)\")\n",
    "    \n",
    "    # Display column information\n",
    "    print(\"\\nColumns info:\")\n",
    "    print(df.info())\n",
    "else:\n",
    "    print(f\"Error: The file {train_data_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ae92c-45be-4e19-9013-cf8360ced8a7",
   "metadata": {},
   "source": [
    "The next step is data preprocessing. The `process` function below is used to prepare the credit risk data for training with XGBoost. It performs several important steps to get the data ready:\n",
    "- it encodes categorical features (like loan purpose and credit history) using one-hot encoding, which creates separate columns for each category.\n",
    "- it separates the features from the target variable (credit risk), and encodes the target labels as numbers.\n",
    "- it fits the featurizer model and transforms the datasets\n",
    "- it splits the data into training and validation sets\n",
    "\n",
    "   \n",
    "All processed datasets are saved as CSV files. Additionally, the function saves the featurizer model and uploads it to Amazon S3. This ensures you can apply the exact same transformations to new data when making predictions during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121887a9-136d-4202-882e-a18d4d548696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import tarfile\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import strftime, gmtime\n",
    "from typing import Tuple, Dict\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define some helper functions\n",
    "def save_to_csv(data: np.ndarray, filepath: str) -> None:\n",
    "    pd.DataFrame(data).to_csv(filepath, header=False, index=False)\n",
    "\n",
    "def upload_to_s3(local_path: str, s3_path: str) -> None:\n",
    "    try:\n",
    "        S3Uploader.upload(local_path, s3_path)\n",
    "        print(f\"âœ“ Uploaded to {s3_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload to S3: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_s3_link(bucket_name: str, prefix: str, model_dir: str) -> str:\n",
    "    region = boto3.session.Session().region_name\n",
    "    return (f\"https://s3.console.aws.amazon.com/s3/buckets/\"\n",
    "            f\"{bucket_name}/{prefix}/{model_dir}/?region={region}&tab=objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a1d40-318f-474f-a874-2ac6726b0f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, bucket_name: str, prefix: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, str]:\n",
    "    \"\"\"\n",
    "    Preprocess credit risk data for machine learning model training.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe containing credit risk data\n",
    "        bucket_name: S3 bucket name to store artifacts\n",
    "        prefix: S3 prefix path for organizing artifacts\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_val, y_train, y_val: train and validation datasets and labels\n",
    "        model_dir: local path to model parameters\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Configuration constants\n",
    "        categorical_cols = [\n",
    "            \"credit_history\",\n",
    "            \"purpose\",\n",
    "            \"personal_status_sex\",\n",
    "            \"other_debtors\",\n",
    "            \"property\",\n",
    "            \"other_installment_plans\",\n",
    "            \"housing\",\n",
    "            \"job\",\n",
    "            \"telephone\",\n",
    "            \"foreign_worker\"\n",
    "        ]\n",
    "        \n",
    "        target_column = \"credit_risk\"\n",
    "        split_ratio = 0.3\n",
    "        random_state = 42\n",
    "        # Create timestamp and directories\n",
    "        timestamp = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "        print(f\"Starting preprocessing with timestamp: {timestamp}\")\n",
    "\n",
    "        output_path = f\"output/pre-process-{timestamp}/data\"\n",
    "        model_dir = f\"output/pre-process-{timestamp}/model/sklearn\"\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "        # Validate input data\n",
    "        missing_cols = [col for col in categorical_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "        if target_column not in df.columns:\n",
    "            raise ValueError(f\"Target column {target_column} not found\")\n",
    "\n",
    "        # Create and configure transformer\n",
    "        transformer = make_column_transformer(\n",
    "            (OneHotEncoder(sparse_output=False, handle_unknown='ignore'), \n",
    "             categorical_cols),\n",
    "            remainder=\"passthrough\"\n",
    "        )\n",
    "\n",
    "        # Prepare features and target\n",
    "        X = df.drop(target_column, axis=1)\n",
    "        y = df[target_column]\n",
    "\n",
    "        # Log class distribution\n",
    "        class_dist = y.value_counts(normalize=True).round(3) * 100\n",
    "        print(f\"Class distribution:\\n{class_dist}\")\n",
    "\n",
    "        # Transform features\n",
    "        print(\"Transforming features...\")\n",
    "        featurizer_model = transformer.fit(X)\n",
    "        features = featurizer_model.transform(X)\n",
    "        labels = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        # Split dataset\n",
    "        print(f\"Splitting data with {split_ratio:.0%} validation ratio...\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            features, labels,\n",
    "            test_size=split_ratio,\n",
    "            random_state=random_state,\n",
    "            stratify=labels\n",
    "        )\n",
    "\n",
    "        # Log dataset shapes\n",
    "        print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n",
    "\n",
    "        # Save datasets\n",
    "        datasets = {\n",
    "            \"train_features.csv\": X_train,\n",
    "            \"train_labels.csv\": y_train,\n",
    "            \"val_features.csv\": X_val,\n",
    "            \"val_labels.csv\": y_val\n",
    "        }\n",
    "        \n",
    "        for filename, data in datasets.items():\n",
    "            save_to_csv(data, os.path.join(output_path, filename))\n",
    "\n",
    "        # Save and archive model\n",
    "        model_path = os.path.join(model_dir, \"model.joblib\")\n",
    "        model_archive = os.path.join(model_dir, \"model.tar.gz\")\n",
    "        \n",
    "        print(\"Saving feature transformer model...\")\n",
    "        joblib.dump(featurizer_model, model_path)\n",
    "        \n",
    "        with tarfile.open(model_archive, \"w:gz\") as tar:\n",
    "            tar.add(model_path, arcname=\"model.joblib\")\n",
    "        print(f\"Model archived to {model_archive}\")\n",
    "\n",
    "        # Upload to S3\n",
    "        s3_path = f\"s3://{bucket_name}/{prefix}/{model_dir}\"\n",
    "        upload_to_s3(model_archive, s3_path)\n",
    "\n",
    "        print(\"Preprocessing completed successfully!\")\n",
    "        \n",
    "        # Display S3 link\n",
    "        from IPython.display import display, HTML\n",
    "        s3_link = create_s3_link(bucket_name, prefix, model_dir)\n",
    "        display(HTML(f'<b>Click the link to navigate to the outputs in Amazon S3:</b> <a href=\"{s3_link}\" target=\"_blank\">View</a>'))\n",
    "\n",
    "        return X_train, X_val, y_train, y_val, model_dir\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Preprocessing failed: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90a8b7-d51d-455c-b43d-a32131118cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "X_train, X_val, y_train, y_val, featurizer_model_dir = preprocess(df, bucket_name, model_prefix)\n",
    "\n",
    "# Store this as you will need it in subsequent notebooks\n",
    "\n",
    "%store featurizer_model_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a834af-aaca-4927-af2d-1a32e5410647",
   "metadata": {},
   "source": [
    "## 3.  Train the XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2bb7e-4d4a-4772-b325-ae2573e85e75",
   "metadata": {},
   "source": [
    "You are now ready to train the model. The`train`function below uses some of the XGBoost hyperparameters as input and runs the training. The model artifacts are saved in Amazon S3 for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bf85c-844f-498f-89db-71d24294ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(bucket_name, prefix, X, val_X, y, val_y, num_round=100, params=None, early_stopping_rounds=10,):\n",
    "    \"\"\"\n",
    "    Train an XGBoost model with validation and save the model artifact.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Training features\n",
    "    val_X : array-like\n",
    "        Validation features\n",
    "    y : array-like\n",
    "        Training labels\n",
    "    val_y : array-like\n",
    "        Validation labels\n",
    "    num_round : int, default=100\n",
    "        Number of boosting rounds\n",
    "    params : dict, default=None\n",
    "        XGBoost parameters\n",
    "    early_stopping_rounds : int, default=10\n",
    "        Stop training if validation performance doesn't improve\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Model training results including the model and evaluation metrics\n",
    "    \"\"\"\n",
    "    # Set default parameters if none provided\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'max_depth': 5,\n",
    "            'eta': 0.1\n",
    "        }\n",
    "    \n",
    "    # Create a timstamp for the current time\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "    # Create output directory\n",
    "    model_dir = f\"output/training-{timestamp}/model\"\n",
    "    try:\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        print(f\"Directory '{model_dir}' created successfully.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory '{model_dir}': {e}\")\n",
    "        \n",
    "    # Create DMatrix objects (only once)\n",
    "    dtrain = xgboost.DMatrix(X, label=y)\n",
    "    dval = xgboost.DMatrix(val_X, label=val_y)\n",
    "\n",
    "    # Set up evaluation watchlist\n",
    "    watchlist = [(dtrain, \"train\"), (dval, \"validation\")]\n",
    "\n",
    "    # Dictionary to store evaluation results\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    bst = xgboost.train(\n",
    "        params=params, \n",
    "        dtrain=dtrain, \n",
    "        evals=watchlist, \n",
    "        num_boost_round=num_round,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        evals_result=evaluation_results\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_preds = bst.predict(dval)\n",
    "    auc_score = roc_auc_score(val_y, val_preds)\n",
    "    print(f\"Validation AUC: {auc_score:.4f}\")\n",
    "    \n",
    "    # Save model \n",
    "    model_path = os.path.join(model_dir, \"model.ubj\")\n",
    "    bst.save_model(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    # Compress the model artifact to a tar file\n",
    "    model_archive = os.path.join(model_dir, \"model.tar.gz\")\n",
    "    with tarfile.open(model_archive, \"w:gz\") as tar:\n",
    "        tar.add(model_path, arcname=\"model.ubj\")\n",
    "    print(f\"Model archived to {model_archive}\")\n",
    "\n",
    "     # Upload to Amazon S3\n",
    "    s3_path = f\"s3://{bucket_name}/{prefix}/{model_dir}\"\n",
    "    upload_to_s3(model_archive, s3_path)\n",
    "\n",
    "    print(\"Training completed successfully!\")\n",
    "    \n",
    "    # Display S3 link\n",
    "    from IPython.display import display, HTML\n",
    "    s3_link = create_s3_link(bucket_name, prefix, model_dir)\n",
    "    display(HTML(f'<b>Click the link to navigate to the outputs in Amazon S3:</b> <a href=\"{s3_link}\" target=\"_blank\">View</a>'))\n",
    "\n",
    "    # Return model and metrics\n",
    "    return {\n",
    "        \"model\": bst,\n",
    "        \"auc_score\": auc_score,\n",
    "        \"evaluation_results\": evaluation_results,\n",
    "        \"s3_model_path\": f\"{s3_path}/model.tar.gz\",\n",
    "        \"archive_path\": model_archive\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a79dc69-856a-4b04-9001-933b68011237",
   "metadata": {},
   "source": [
    "We are now ready to call the *train* method with the training and validation datasets. The method returns a dictionary containing the trained model, evaluation metrics, and file paths to the saved model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44e86b-cc3d-4d71-afff-13873da0f382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.1\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"silent\": \"1\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\",\n",
    "    \"subsample\": \"0.8\",\n",
    "    \"eval_metric\": \"auc\"\n",
    "}\n",
    "num_round = 50\n",
    "\n",
    "training_results = train(bucket_name, model_prefix, X_train, X_val, y_train, y_val,num_round, hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2abad7-ea1c-4da5-b7a8-01b0f14bc987",
   "metadata": {},
   "source": [
    "## 4.  Analyse the model results\n",
    "\n",
    "Use the `matplotlib` library to visualize the AUC score (measures model's ability to distinguish between good and bad credit risks) for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bbad9e-72f5-4f13-8dc5-bc0e545afb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = training_results[\"model\"]\n",
    "\n",
    "# Extract and store the S3 model artifact URI\n",
    "model_artifact = training_results[\"s3_model_path\"]\n",
    "print(model_artifact)\n",
    "%store model_artifact\n",
    "\n",
    "# Print the validation AUC score\n",
    "validation_auc=training_results['auc_score']\n",
    "print(f\"Validation AUC: {validation_auc}\")\n",
    "\n",
    "# Plot the training and validation metrics\n",
    "results = training_results[\"evaluation_results\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results['train']['auc'], label='Train AUC')\n",
    "plt.plot(results['validation']['auc'], label='Validation AUC')\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('XGBoost Training Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804601e-d5b3-4cbd-bb30-ff2bbda74d69",
   "metadata": {},
   "source": [
    "## 6. (Optional) Save the model to the SageMaker model registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a361788-53de-47e6-a17d-4b06574f9e5a",
   "metadata": {},
   "source": [
    "[Amazon SageMaker Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html) is a capability that helps organize and manage machine learning models in production environments.  It allows you to catalog models, manage different versions, track metadata (like training metrics), and maintain model lineage for traceability. \n",
    "\n",
    "The model registry is structured with **model package groups** that contain different versions of the model called **model packages**. You can organize model package groups into _Collections_ for better management. The Model Registry integrates with SageMaker's MLOps tools, enabling automated model deployment through CI/CD pipelines and facilitating collaboration across teams. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd43bd-6c09-48f1-9bcb-2a72599275a7",
   "metadata": {},
   "source": [
    "In this section, you will register the featurizer and XGboost models you trained in the previous steps to the model registry. First, you create a new model package group for the credit risk prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd47a9-24d6-4e94-99e6-6e085a75093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model package group name\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "model_package_group_name = f\"credit-risk-{timestamp}\"\n",
    "\n",
    "# Create the model package group\n",
    "try:\n",
    "    response = sagemaker_client.create_model_package_group(\n",
    "        ModelPackageGroupName=model_package_group_name,\n",
    "        ModelPackageGroupDescription=\"Credit risk prediction models based on XGBoost\",\n",
    "        Tags=[\n",
    "            {\n",
    "                'Key': 'Project',\n",
    "                'Value': 'CreditRiskPrediction'\n",
    "            },\n",
    "            {\n",
    "                'Key': 'Framework',\n",
    "                'Value': 'XGBoost'\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Get the ARN of the model package group\n",
    "    model_package_group_arn = response['ModelPackageGroupArn']\n",
    "    \n",
    "    print(f\"Successfully created Model Package Group: {model_package_group_name}\")\n",
    "    print(f\"Model Package Group ARN: {model_package_group_arn}\")\n",
    "    \n",
    "    # Store the model package group name for later use\n",
    "    # This can be used when registering model versions\n",
    "    model_registry_details = {\n",
    "        \"model_package_group_name\": model_package_group_name,\n",
    "        \"model_package_group_arn\": model_package_group_arn\n",
    "    }\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating model package group: {str(e)}\")\n",
    "\n",
    "# Return the model package group name\n",
    "model_package_group_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0add4-8605-46e3-b39b-c4bc00b2eee9",
   "metadata": {},
   "source": [
    "Now that the model package group is ready, you can start registering model packages to it.  A **model package** contains information about the model version itself and details for deployment such as the location of the model artifacts, metadata, the URI of the container image to be used during deployment, the SageMaker instance types, the model approval status, and more.  \n",
    "\n",
    "The following code registers the model to the model registry by adding a model package to the model package group created earler.\n",
    "\n",
    "When you deploy models on Amazon SageMaker, you can choose to use AWS pre-built container images, extend them, or bring your own container image. In this lab, you will use the **sagemaker distribution image** and specifically the same version as the one that powers your JupyterLab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b526c42-d9f1-495c-ba58-13f79585316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current region\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Dynamically fetch the ECR image URI for XGBoost\n",
    "image_uri = os.environ['SAGEMAKER_INTERNAL_IMAGE_URI']\n",
    "\n",
    "# Create base model package input\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageGroupName\": model_package_group_arn,\n",
    "    \"ModelPackageDescription\": \"Model to detect credit risk\",\n",
    "    \"ModelApprovalStatus\": \"PendingManualApproval\",\n",
    "    \"Domain\": \"MACHINE_LEARNING\",\n",
    "    \"Task\": \"CLASSIFICATION\",\n",
    "    \"CustomerMetadataProperties\": {\n",
    "        \"ProjectName\": \"CreditRiskPrediction\",\n",
    "        \"ModelType\": \"XGBoost\",\n",
    "        \"BusinessProblem\": \"Credit Risk Assessment\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create inference specification with dynamically fetched image URI\n",
    "inference_specification = {\n",
    "    \"InferenceSpecification\": {\n",
    "        \"Containers\": [\n",
    "            {\n",
    "                \"Image\": image_uri,\n",
    "                \"ModelDataUrl\": training_results[\"s3_model_path\"]\n",
    "            }\n",
    "        ],\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": [\n",
    "            \"ml.t2.medium\", \n",
    "            \"ml.m5.large\", \n",
    "            \"ml.m5.xlarge\"\n",
    "        ],\n",
    "        \"SupportedContentTypes\": [\"text/csv\"],\n",
    "        \"SupportedResponseMIMETypes\": [\"text/csv\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Update the input dict with inference specification\n",
    "create_model_package_input_dict.update(inference_specification)\n",
    "\n",
    "# Create the model package\n",
    "create_model_package_response = sagemaker_client.create_model_package(**create_model_package_input_dict)\n",
    "model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "print('ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d5f4f-d4e5-4905-8089-283f598e1df0",
   "metadata": {},
   "source": [
    "### Conclusion and Next Steps:\n",
    "- You preprocessed the data\n",
    "- The trained an XGboost model \n",
    "- You saved the outputs to the model registry\n",
    "- You're ready for Module 2!\n",
    "\n",
    "In Module 2, you'll use the model you trained to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297144ad-0f12-4f14-a946-f620cc223e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
